# -*- coding: utf-8 -*-
"""neural_style_transfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14JbtjmRUhrGqaq-H8Zyp8qnqSOrnKbFH
"""

import os
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import PIL.Image
import time
import functools
from PIL import Image
import random

#use GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/DeepLearn/datasets/NST_data.zip" >/dev/null

content_images = '/content/NST_data/content_images'
style_images = '/content/NST_data/style_images'

def load_img(folder, img_names):
  list_images = []
  for img_nm in img_names:
    if img_nm=='.DS_Store': continue
    path_to_img = folder+'/'+img_nm
    max_dim = 512
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, size= (max_dim,max_dim))
    img = img[tf.newaxis, :]
    list_images.append(img)
  return list_images

content_names = os.listdir(content_images)
style_names = os.listdir(style_images)

cont_imgs = load_img(content_images, content_names)
style_imgs = load_img(style_images, style_names)

"""*Neural Style Transfer (NTS) is performed by merging two images together in such a way to have the same content of an image (content image) but with the style of another (style image).*"""

def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)
    plt.imshow(image)
  if title:
    plt.title(title)

for i in range(3):
  plt.figure(figsize=(10, 10))
  content = cont_imgs[i]
  style = style_imgs[i]
  plt.subplot(1, 2, 1)
  imshow(content, 'Content Image')
  plt.axis("off")
  plt.subplot(1, 2, 2)
  imshow(style, 'Style Image')
  plt.axis("off")

"""

* Let's import a pre-trained VGG network already trained to recognize a variety of low-level features, in the shallow layers, and high-level features, in deeper layers.
*  The idea is to match the **content of the generated image G with the content of the content image C**.
*  Since G and C should have similar content, the solution is to chose and **intermediate layer** in order to ensure that the network detect both **lower and higer level features**. This should give the most visually pleasant results.
* Insted, for **Style layer** is good to select many of them due to the fact we could achieve better results if we merge style cost coming from different layers."""

vgg_base = tf.keras.applications.VGG19(include_top=False,weights='imagenet')
conv_layers = []
content_layers = []
style_layers = []
for layer in vgg_base.layers:
  name = layer.name.split('_')
  if name[-1].find('conv') == 0:
    conv_layers.append(layer.name)
print('VGG19 conv layers: \n', conv_layers)
#chose intermediate layers from the pre trained VGG to represent content and style of the image
content_layers.append((conv_layers[-1],1))
print(' Content layers: \n', content_layers)
for conv_layer in conv_layers:
  name_c = conv_layer.split('_')
  if name_c[1] == 'conv1': style_layers.append((conv_layer,0.2))
print(' Styile layers: \n', style_layers)

"""#Model"""

def layers_extractor(layers,vgg):
  vgg.trainable = False
  #extract only the layers defined in style and content.
  outputs = [vgg.get_layer(layer[0]).output for layer in layers]
  model = tf.keras.Model([vgg.input], outputs)
  return model

vgg_out = layers_extractor(style_layers + content_layers, vgg_base)

#random select content and style image.
def select_C_S(content_images,style_images):
  content_example_image = random.choice((content_images))
  style_example_image = random.choice((style_images))
  plt.figure(figsize=(10, 10))
  plt.subplot(1, 2, 1)
  imshow(content_example_image, 'Content Image')
  plt.axis("off")
  plt.subplot(1, 2, 2)
  imshow(style_example_image, 'Style Image')
  plt.axis("off")

  return content_example_image, style_example_image

cont_img, style_img = select_C_S(cont_imgs,style_imgs)

"""#Cost"""

#Set a_C to be the tensor giving the hidden layer activation for layer "block5_conv4" using the content image
prep_content = tf.Variable(cont_img)
a_C = vgg_out(prep_content)
#a_S is the tensor giving the hidden layer activation for style using our style image.
prep_style =  tf.Variable(style_img)
a_S = vgg_out(prep_style)

"""**Content Cost**


*   Measure how similar are two activations in a certain hidden layer for both the content image C and the generated image G. 
*   To simplify the computation we unroll the 3D volumnes into a 2D matrix

"""

def content_cost(C_output, G_output):
  #activations of the content
  a_C = C_output[-1]
  #activations of the generated
  a_G = G_output[-1]
  #get shape
  m, n_H, n_W, n_C = a_G.get_shape().as_list()
  #unrolling operation
  a_C_u = tf.reshape(a_C, shape=[m, n_H*n_W, n_C])
  a_G_u = tf.reshape(a_G, shape=[m, n_H*n_W, n_C])
  #content cost computation
  J_content = 1/(4*n_H*n_W*n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_u,a_G_u)))

  return J_content

"""**Gram Matrix (Style Matrix S)**


*   Used to compute the **style cost**
*   **$G_{gram}$** it is obtained by computing dot products of a set of vectors `(v_1...v_n). In fact the entries of this matrix are *$G_{gram_{i,j}} = v_i^Tv_j$*
* Basically $G_{gram_{i,j}}$ compares how similar $v_i$ is to $v_j$. If they are highly similar we get a large dot product, otherwise it will be small. In this way it is possibile to compute the correlation between the activations of a filter $i$ and a filter $j$.
* The Gram matrix is computed for both the style image S and the generated image G.

"""

def Style_Matrix(A_mat):
  G_gram = tf.matmul(A_mat,tf.transpose(A_mat))
  return G_gram

"""**Style cost**


* Style matrix captures the style for both style image S and generated image G. 
*   It turns out the it is possibile to obtain more pleasant results ig we use the style cost function from **multiple different layers**


"""

#compute style cost for a single layer
def layer_style_cost(activation_S, activation_G):
  #get the shape of the two activations
  m,n_H,n_W,n_C = activation_G.get_shape().as_list()
  #unroll the two activations to obtain a 2D matrix for each of them
  a_G_u = tf.transpose(tf.reshape(activation_G, shape=[n_H * n_W, n_C]))
  a_S_u = tf.transpose(tf.reshape(activation_S, shape=[n_H * n_W, n_C]))
  
  #compute gram matrix
  G_G = Style_Matrix(a_G_u)
  G_S = Style_Matrix(a_S_u)
  #compute style cost
  J_style_layer = 1./(4*n_C**2 *(n_H*n_W)**2)*tf.reduce_sum(tf.pow((G_S - G_G), 2))
  return J_style_layer

"""

*   Let's combine costs from different style layers so to obtain the overall style cost function

"""

def style_cost(S_out, G_out, list_style_layers):
  J_style = 0
  #the last element of the array contains the content image so we do not select it
  #a_S is set to be the hidden layer activation from the layer we have selected
  a_S = S_out[:-1]
  #same for a_G, i.e, the output of the choosen hidden layer.
  a_G = G_out[:-1]
  for i, weight in zip(range(len(a_S)), list_style_layers):  
    # Compute style_cost for the current layer
    J_style_layer = layer_style_cost(a_S[i], a_G[i])
    # Add weight * J_style_layer of this layer to overall style cost
    J_style += weight[1] * J_style_layer
  return J_style

"""**Total Cost**


*   The overall cost is defined in such a way to minimize both the style and the content cost.
* $α$ and $β$ are hyperparameter weighting the importance of  both the content and style cost.
"""

def cost(J_content, J_style, alpha = 10, beta = 40):
  J = alpha*J_content+beta*J_style
  return J

"""#Train

* Initialize the generated image G slightly correlated with the content image C.
* This will help the content of the "generated" image more rapidly match the content of the "content" image.
"""

#utility to display the image
def clip(img):
  return tf.clip_by_value(img, clip_value_min=0.0, clip_value_max=1.0)
  
def tensor_to_image(img):
  img = img * 255
  img = np.array(img, dtype=np.uint8)
  if np.ndim(img) > 3:
    assert img.shape[0] == 1
    img = img[0]
  return Image.fromarray(img)

#initialize random noise
#let's frist try for the first image in our dataset
G_image = tf.Variable(cont_img)
noise_image = tf.random.uniform(tf.shape(cont_img), -0.25, 0.25)
G_image = tf.add(G_image, noise_image)
G_image = tf.clip_by_value(G_image, clip_value_min=0.0, clip_value_max=1.0)
print('generated image shape: ', G_image.shape)
plt.imshow(G_image.numpy()[0])
plt.show()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99)

def train_step(generated_image, style_layers):
  with tf.GradientTape() as tape:
    # Compute a_G as the vgg_model_outputs for the current generated image
    a_G = vgg_out(generated_image)
    # Compute the style cost
    J_style = style_cost(a_S, a_G, style_layers)
    # Compute the content cost
    J_content = content_cost(a_C, a_G)
    # Compute the total cost
    J = cost(J_content, J_style) 
        
  grad = tape.gradient(J, generated_image)
  optimizer.apply_gradients([(grad, generated_image)])
  generated_image.assign(clip(generated_image))
  return J

epochs = 20001
G_image = tf.Variable(G_image)
for i in range(epochs):
  train_step(G_image,style_layers)
  if i % 250 == 0:
    print(f"Epoch {i} ")
  if i % 250 == 0:
    image = tensor_to_image(G_image)
    plt.imshow(image)
    plt.axis("off")
    plt.show() 

image.save(f"/content/NST_data/image_{i}.jpg")
image.save(f"/content/drive/MyDrive/DeepLearn/datasets/image_{i}.jpg")